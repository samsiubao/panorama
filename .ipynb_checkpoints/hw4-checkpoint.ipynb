{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4, Computer Vision COMS 4731 (Due Nov. 7, 2018, 2:40 PM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">**Sam Siu ss4313:**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________\n",
    "### What you need to submit:\n",
    "Your submission should include this iPython notebook (titled <span style=\"color:red\">&lt;uni&gt;.ipynb</span>, run through once before submission), `output/` (which should include `test1.png`, `test2.png`, `test3.png`, `test4a.png`, `test4b.png`, and `test5.png`), and a `part6/` directory (which should include input pictures you used for part 6, as well as the output stitched image).\n",
    "_________________________________\n",
    "\n",
    "### Guidelines\n",
    "**Refer to the PDF for expected results.**\n",
    "\n",
    "For submission, please <span style=\"color:red\">**re-run the notebook and save once all cells are finished running**</span>. This will allow us to see your results immediately, and verify that everything works on your machine at submission time.\n",
    "\n",
    "**You may NOT use any functions from cv2, scipy for your submission, unless specified.**\n",
    "\n",
    "You are encouraged to use **numpy** for indexing, doing calculations, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each part will create an additional output file, in the `output` directory. Please submit these in addition to this notebook, re-run on all the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "\n",
    "def write_output_img(filename, img):\n",
    "    if not os.path.isdir(\"output\"):\n",
    "        os.mkdir(\"output\")\n",
    "\n",
    "    cv2.imwrite(\"output/\" + filename, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________\n",
    "## Image Stitching\n",
    "\n",
    "Your task is to develop a stitching algorithm that stitches a collection of photos into a mosaic. Before we create the mosaicking app, we will create the individual tools required to build it. Each tool is a separate function you need to fill out below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Compute homography (4 pt)\n",
    "\n",
    "In this part, the goal is to develop a function to calculate and apply the homography between a pair of images. We will also develop two additional small programs to help verify whether the calculated homography is correct.\n",
    "\n",
    "We have already implemented for you the function `compute_homography()` that calculates the homography between two sets of corresponding points in two images. Your task to implement the function `apply_homography()` that applies a homography to a set of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_homography(src, dst):\n",
    "    # This function computes the homography from src to dst.\n",
    "    #\n",
    "    # Input:\n",
    "    #     src: source points, shape (n, 2)\n",
    "    #     dst: destination points, shape (n, 2)\n",
    "    # Output:\n",
    "    #     H: homography from source points to destination points, shape (3, 3)\n",
    "    \n",
    "\n",
    "    A = np.zeros([2*src.shape[0], 9])\n",
    "    for i in range(src.shape[0]):\n",
    "        A[2*i, :] = np.array([src[i,0],src[i,1],1,0,0,0,\n",
    "                              -dst[i,0]*src[i,0],-dst[i,0]*src[i,1],-dst[i,0]])\n",
    "        A[2*i+1, :] = np.array([0,0,0,src[i,0],src[i,1],1,\n",
    "                                -dst[i,1]*src[i,0],-dst[i,1]*src[i,1],-dst[i,1]])\n",
    "    \n",
    "    w, v = np.linalg.eig(np.dot(A.T, A))\n",
    "    index = np.argmin(w)\n",
    "    H = v[:, index].reshape([3,3])\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def apply_homography(src, H):\n",
    "    dst = []\n",
    "    H = H.flatten()\n",
    "    if src.ndim==1:\n",
    "        src = np.array([src])\n",
    "    for pt in src:    \n",
    "        if pt.ndim == 1:\n",
    "            pt = np.array([pt])       \n",
    "        Z = 1./(H[6]*pt[0,0] + H[7]*pt[0,1] + H[8])\n",
    "        px = (H[0]*pt[0,0] + H[1]*pt[0,1] + H[2])*Z\n",
    "        py = (H[3]*pt[0,0] + H[4]*pt[0,1] + H[5])*Z\n",
    "        dst.append([px,py])\n",
    "    dst = np.array(dst,dtype=int)\n",
    "    return dst\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Backward Warping (8 pt)\n",
    "\n",
    "\n",
    "When we map a source image to its destination image using a homography, we may encounter a problem where multiple pixels of the source image are mapped to the same point of its destination image. What's more, some pixels of the destination image may not be mapped to any pixels of source image. What should we do?\n",
    "\n",
    "Suppose we had homography $H$, source pixel $s$ with coordinates $(x_s, y_s)$, and destination pixel $d$ with coordinates $(x_d, y_d)$. Then, $H \\cdot \\tilde{s} = \\tilde{d}$ (where, $s$, $d$ are in homogenous space).\n",
    "\n",
    "To deal with this problem, we consider a slight caveat, where we map the pixels of the destination image back to source image, and then use the color in the source image as its color. More precisely, for each destination pixel $d = (x_d, y_d)$, we take $H^{-1} \\cdot \\tilde{d}$ to obtain the coordinate of its associated source pixel, $\\tilde{s}$ (from which $s$ can be found). If $s$ is within the bounds of the source image, we take the intensity of $s$ to be the intensity of $d$.\n",
    "\n",
    "Repeating this process over the entire destination image ensures that there are no gaps in the final result. This process is called \"backward warping\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**TODO:**</span> Fill in `backward_warp_img` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import interpolate \n",
    "import numpy as np\n",
    "\n",
    "def backward_warp_img(src_img, H, dst_img_size):\n",
    "    r1 = interpolate.interp2d(np.arange(src_img.shape[1]), np.arange(src_img.shape[0]), src_img[:,:,0])\n",
    "    g1 = interpolate.interp2d(np.arange(src_img.shape[1]), np.arange(src_img.shape[0]), src_img[:,:,1])\n",
    "    b1 = interpolate.interp2d(np.arange(src_img.shape[1]), np.arange(src_img.shape[0]), src_img[:,:,2]) \n",
    "    H = np.linalg.inv(H)   \n",
    "\n",
    "    dst = np.zeros((dst_img_size[0],dst_img_size[1],3), dtype=int)\n",
    "\n",
    "    dst = np.zeros((dst_img_size[0],dst_img_size[1],3), dtype=int)\n",
    "    H = np.linalg.inv(H)   \n",
    "    for x in range(src_img.shape[1]):\n",
    "        for y in range(src_img.shape[0]):\n",
    "            temp  = apply_homography(np.array([x,y]), H)\n",
    "            px,py = temp[0,0], temp[0,1]\n",
    "            if (src_img[y,x] == (0,0,0)).all():\n",
    "                print('nogod')\n",
    "            dst[py,px] = src_img[y,x]\n",
    "            \n",
    "\n",
    "    return dst\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**TODO:**</span> We're going to need a simple mask function which takes the binary mask of an input image. Fill in `binary_mask` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binary_mask(img):\n",
    "    # Input:\n",
    "    #     img: source image, shape (m, n, 3)\n",
    "    # Output:\n",
    "    #     mask: image of shape (m, n) and type 'int'. For pixel [i, j] of mask, if img[i, j] > 0 \n",
    "    #           in any of its channels, mask[i, j] = 1. Else, (if img[i, j] = 0), mask[i, j] = 0.\n",
    "    mask = np.zeros((img.shape[0], img.shape[1]),dtype=int)\n",
    "    count = 0\n",
    "    for x in range(len(img)):\n",
    "        for y in range(len(img[0])):\n",
    "            if (img[x,y]==0).any():\n",
    "                count+=1\n",
    "                mask[x,y] = 0\n",
    "            else:\n",
    "                mask[x,y] = 1\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DO NOT MODIFY:** This will test the homography functions from part 1, as well as `backward_warp_img` and `binary_mask`. It inserts a portrait of Van Gogh (`portrait_small.png`) into the given canvas (`Osaka.png`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - RANSAC (8 pt)\n",
    "\n",
    "Here, you need to implement the RANSAC algorithm to find a good homography as introduced in lecture.\n",
    "\n",
    "The function takes two sets of matched points, $X_s$ and $X_d$ (generated by SIFT) as input, and uses RANSAC to compute the optimal homography. It returns the homography from RANSAC, as well as the indices of points from $X_s$ and $X_d$ that are detected to have very low error when transformed by the homography given by RANSAC.\n",
    "\n",
    "Note, you need to use at least four pairs of matched points to compute a homography."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**TODO:**</span> Fill in `RANSAC` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def RANSAC(Xs, Xd, max_iter, eps):\n",
    "    inliers_idx = []\n",
    "    H = 0\n",
    "    k = 0\n",
    "    loss = math.inf\n",
    "    while(k < max_iter):\n",
    "        temp_inliers = []\n",
    "        src = []\n",
    "        dst = []\n",
    "        for i in range(0,4):\n",
    "            temp  = random.randrange(0,len(Xs))           \n",
    "            src.append(Xs[temp])\n",
    "            dst.append(Xd[temp])\n",
    "        src = np.array(src)\n",
    "        dst = np.array(dst)\n",
    "        \n",
    "        temp_H = compute_homography(src,dst)\n",
    "        temp_dst = apply_homography(Xs, temp_H)\n",
    "        for i in range(len(Xd)):\n",
    "            x = Xd[i][0]\n",
    "            d = distance.euclidean(Xd[i],temp_dst[i])\n",
    "            if d < eps:\n",
    "                Xd[i] = Xd[i].astype(int)\n",
    "                temp_inliers.append(i)\n",
    "                \n",
    "        if len(temp_inliers) > len(inliers_idx):\n",
    "            inliers_idx = temp_inliers\n",
    "            H = temp_H\n",
    "        k += 1\n",
    "    # Input:\n",
    "    #     pts1: the first set of points, shape [n, 2]\n",
    "    #     pts2: the second set of points matched to the first set, shape [n, 2]\n",
    "    #     max_iter: max iteration number of RANSAC\n",
    "    #     eps: tolerance of RANSAC\n",
    "    # Output:\n",
    "    #     inliers_id: the indices of matched pairs when using the homography given by RANSAC\n",
    "    #     H: the homography, shape [3, 3]\n",
    "\n",
    "    return inliers_idx,H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DO NOT MODIFY:** To ease your workload, a function `genSIFTMatchPairs` has been provided.\n",
    "You may have to run: \n",
    "\n",
    "`sudo python3 -m pip install opencv-python==3.4.2.16`\n",
    "\n",
    "`sudo python3 -m pip install opencv-contrib-python==3.4.2.16`\n",
    "\n",
    "and rerun the entire notebook up to this point.\n",
    "\n",
    "(this installs an older version of OpenCV for Python, because SIFT is no longer available in the newer OpenCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def genSIFTMatchPairs(img1, img2):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "    kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "    matches = bf.match(des1, des2)\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "    \n",
    "    pts1 = np.zeros((250,2))\n",
    "    pts2 = np.zeros((250,2))\n",
    "    for i in range(250):\n",
    "        pts1[i,:] = kp1[matches[i].queryIdx].pt\n",
    "        pts2[i,:] = kp2[matches[i].trainIdx].pt\n",
    "    \n",
    "    return pts1, pts2, matches[:250], kp1, kp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DO NOT MODIFY:** This part tests `RANSAC`. The first figure shows the matched points using SIFT; you will see that most of the pairs are well matched, but there are outliers.\n",
    "\n",
    "The second part uses `RANSAC`to remove the outliers. You may try different `max_iter` and `eps` values to achieve better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Blending an Image Pair (4 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement `blend_image_pair`, which blends two images given their binary masks.\n",
    "\n",
    "There is also a blending mode parameter, `mode`, which can be `overlay` or `blend`. The `overlay` setting should copy `dst_img` over `src_img` wherever the `dst_img` applies. The `blend` setting should perform weighted blending as discussed in class.\n",
    "\n",
    "You may use **scipy.ndimage.morphology.distance_transform_edt** to compute a new weighted mask for blending."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**TODO:**</span> Fill in `blend_image_pair` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.morphology import distance_transform_edt as euc_dist\n",
    "\n",
    "def blend_image_pair(src_img, src_mask, dst_img, dst_mask, mode):\n",
    "\n",
    "    a = euc_dist(src_mask)\n",
    "    b = euc_dist(dst_mask)\n",
    "    output = src_img + dst_img\n",
    "    if mode == 'overlay':\n",
    "        for x in range(output.shape[0]):\n",
    "            for y in range(output.shape[1]):\n",
    "                if (output[x,y] != src_img[x,y] ).all():\n",
    "                    output[x,y]-=  src_img[x,y]\n",
    "        return output\n",
    "\n",
    "    if mode == 'blend':\n",
    "        for x in range(output.shape[0]):\n",
    "            for y in range(output.shape[1]):\n",
    "                if (a[x,y]+b[x,y]) == 0:\n",
    "                    alpha = 0\n",
    "                else:\n",
    "                    alpha = a[x,y]/(a[x,y]+b[x,y])\n",
    "                output[x,y] =  alpha*src_img[x,y] + (1-alpha)*dst_img[x,y]\n",
    "\n",
    "        output= np.uint8(output)  \n",
    "        return output\n",
    "    \n",
    "    # Given two images and their binary masks, the two images are blended.\n",
    "    # \n",
    "    # Input:\n",
    "    #     src_img: First image to be blended, shape (m, n, 3)\n",
    "    #     src_mask: src_img's binary mask, shape (m, n)\n",
    "    #     dst_img: Second image to be blended, shape (m, n, 3)\n",
    "    #     dst_mask: dst_img's binary mask, shape (m, n)\n",
    "    #     mode: Blending mode, either \"overlay\" or \"blend\"\n",
    "    # Output:\n",
    "    #     Blended image of shape (m, n, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Image Stitching (12 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have all the tools to build the stitching app! Write a program `stitch_img` that stitches the input images into one mosaic.\n",
    "\n",
    "Your program should accept an arbitrary number of images. You can assume the order of the input images matches the order you wish to stitch the images. Also, in this assignment we will only stitch images horizontally. Use the `blend` mode to blend the image when you call `blend_image_pair`.\n",
    "\n",
    "**Hint: When warping multiple images on to a single base image, you may want to first compute the bounding box. This bounding box may extend beyond the size of the base image and can have negative coordinates. How would you warp the images in this case? Hint Hint: Homography needs to be updated with additional translation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**TODO:**</span> Fill in `stitch_img` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def left_right_H(left,center,right):\n",
    "    left_img, center_img, right_img = left,center, right\n",
    "\n",
    "    left_1, left_2, matches, kp1, kp2 = genSIFTMatchPairs(left_img, center_img)\n",
    "    inliers_idx, left_H = RANSAC(left_1, left_2, 500, 10)\n",
    "    \n",
    "    top,bottom,left,right = get_padding(left_img,H)\n",
    "    \n",
    "    rt_1, rt_2, matches, kp1, kp2 = genSIFTMatchPairs(right_img, center_img)\n",
    "    inliers_idx, right_H = RANSAC(pts1, pts2, 500, 10)  \n",
    "    return left_H, right_H\n",
    "\n",
    "def get_padding(img, H):\n",
    "#     print(img.shape)\n",
    "    pt1 = [0,0]\n",
    "    pt2 = [img.shape[1]-1,0]\n",
    "    pt3 = [ img.shape[1]-1,img.shape[0]-1 ]\n",
    "    pt4 = [ 0 ,img.shape[0]-1]\n",
    "    corners = np.array([pt1,pt2,pt3,pt4])\n",
    "    corners = np.matrix(corners)\n",
    "#     print(corners)\n",
    "#     test_pts = np.matrix('0, 0; 0, 719; 1058,719; 1058,0')\n",
    "#     print(test_pts)\n",
    "    match_pts = apply_homography(corners, (H))\n",
    "#     print(match_pts)\n",
    "    top = img.shape[0]\n",
    "    top, bottom, left, right = 0,0,0,0\n",
    "    for x in match_pts:\n",
    "        if x[0]<left:\n",
    "            left = (x[0])\n",
    "        if x[1]<bottom:\n",
    "            bottom = (x[1])\n",
    "        if x[1]>top:\n",
    "#             print('top' , top)\n",
    "            top =  x[1] \n",
    "        if x[0]>right:\n",
    "            right = x[0]\n",
    "    left = abs(left)\n",
    "    bottom = abs(bottom)\n",
    "    top = top - img.shape[0]\n",
    "\n",
    "    top = abs(top) if top > 0 else 0\n",
    "\n",
    "#     print(top, img.shape[0])\n",
    "#     top = top - img.shape[0]\n",
    "    \n",
    "    return(t,bottom, left, right)\n",
    "# center_img = cv2.imread(\"mountain_center.png\")\n",
    "# left_img = cv2.imread(\"mountain_left.png\")\n",
    "# right_img = cv2.imread(\"mountain_right.png\")\n",
    "# pts1, pts2, matches, kp1, kp2 = genSIFTMatchPairs(center_img, right_img)\n",
    "# inliers_idx, right_H = RANSAC(pts1, pts2, 500, 10)\n",
    "# right_H = np.linalg.inv(right_H)\n",
    "# bottom,top,left,right = get_padding(right_img,right_H)\n",
    "# right = right - center_img.shape[1]\n",
    "# print(bottom,top,right,left)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**TODO:**</span> Run your code on the given images. You can modify the indicated line below so that `stitch_img` takes in the images in the order that you would like them to be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_right(center_img, right_img):\n",
    "    pts1, pts2, matches, kp1, kp2 = genSIFTMatchPairs(center_img, right_img)\n",
    "    inliers_idx, right_H = RANSAC(pts1, pts2, 500, 10)\n",
    "    right_H = np.linalg.inv(right_H)\n",
    "    bottom,top,left,right = get_padding(right_img,right_H)\n",
    "    \n",
    "    right = right - center_img.shape[1]\n",
    "\n",
    "    right = abs(right) if right > 0 else 0\n",
    "#     print(bottom,top,right,left)\n",
    "\n",
    "    base = cv2.copyMakeBorder( center_img, top, bottom, 0, right,cv2.BORDER_CONSTANT)\n",
    "#     print('base')\n",
    "#     plt.imshow(cv2.cvtColor(base.astype(\"uint8\"), cv2.COLOR_BGR2RGB))\n",
    "#     plt.show()\n",
    "#     print('right')\n",
    "#     plt.imshow(cv2.cvtColor(right_img.astype(\"uint8\"), cv2.COLOR_BGR2RGB))\n",
    "#     plt.show()\n",
    "    M = np.float32([[1,0,-left],[0,1,top],[0,0,1]])\n",
    "    new_right_H = np.zeros(H.shape)\n",
    "    np.matmul(M,right_H,new_right_H)\n",
    "    new_right = cv2.warpPerspective(right_img, (new_right_H), (base.shape[1], base.shape[0]))\n",
    "#     print('new right')\n",
    "#     plt.imshow(cv2.cvtColor(new_right, cv2.COLOR_BGR2RGB))\n",
    "#     plt.show()\n",
    "    id_M = np.float32([[1,0,0],[0,1,0],[0,0,1]])\n",
    "    new_base = cv2.warpPerspective(base, (id_M), (base.shape[1], base.shape[0]))\n",
    "\n",
    "    new_blend_right = blend_image_pair(new_base, binary_mask(new_base), new_right, binary_mask(new_right), \"blend\")\n",
    "    new_blend_right =new_blend_right.astype('uint8')\n",
    "    return new_blend_right, top\n",
    "\n",
    "def stitch_left(center_img,left_img,t, new_blend):\n",
    "#     print('left')\n",
    "#     plt.imshow(cv2.cvtColor(left_img.astype('uint8'), cv2.COLOR_BGR2RGB))\n",
    "#     plt.show()\n",
    "    pts1, pts2, matches, kp1, kp2 = genSIFTMatchPairs(left_img, center_img)\n",
    "    inliers_idx, H = RANSAC(pts1, pts2, 500, 10)\n",
    "    \n",
    "    nmatch_pts = apply_homography(corners, (H))\n",
    "#     print('mt', nmatch_pts)\n",
    "    \n",
    "    bottom,top,left,right = get_padding(left_img,H)\n",
    "    n_base = cv2.copyMakeBorder(new_blend,  top, 0, left, 0,cv2.BORDER_CONSTANT)\n",
    "    M = np.float32([[1,0,left],[0,1,top+t],[0,0,1]])\n",
    "    left_H = np.zeros(H.shape)\n",
    "    np.matmul(M,H,left_H)\n",
    "    new_left = cv2.warpPerspective(left_img, (left_H), (n_base.shape[1], n_base.shape[0]))\n",
    "#     print('newleft')\n",
    "#     plt.imshow(cv2.cvtColor(new_left.astype('uint8'), cv2.COLOR_BGR2RGB))\n",
    "#     plt.show()\n",
    "    new_blend_left = blend_image_pair(n_base, binary_mask(n_base), new_left, binary_mask(new_left), \"blend\")\n",
    "    new_blend_left =new_blend_left.astype('uint8')\n",
    "\n",
    "    return new_blend_left\n",
    "\n",
    "def stitch_img(*imgs):\n",
    "# Step #1: Detect keypoints (DoG, Harris, etc.) and extract local invariant descriptors (SIFT, SURF, etc.) from the two input images.\n",
    "# Step #2: Match the descriptors between the two images.\n",
    "# Step #3: Use the RANSAC algorithm to estimate a homography matrix using our matched feature vectors.\n",
    "# Step #4: Apply a warping transformation using the homography matrix obtained from Step #3.\n",
    "    \n",
    "    if len(imgs)==5:\n",
    "        middle = int(len(imgs)/2)\n",
    "        center_img = imgs[middle]\n",
    "        left_img = imgs[middle-1]\n",
    "        right_img = imgs[middle+1]\n",
    "        center_right,t = stitch_right(center_img, right_img)\n",
    "        \n",
    "        center_img = stitch_left(center_img, left_img, t, center_right)\n",
    "        left_img = imgs[0]\n",
    "        right_img = imgs[4]\n",
    "        center_right,t = stitch_right(center_img, right_img)\n",
    "#         print('new irhgt')\n",
    "#         plt.imshow(cv2.cvtColor(center_right.astype('uint8'), cv2.COLOR_BGR2RGB))\n",
    "#         plt.show()\n",
    "        \n",
    "        final = stitch_left(center_img, left_img, t, center_right)\n",
    "#         print('final')\n",
    "#         plt.imshow(cv2.cvtColor(final.astype('uint8'), cv2.COLOR_BGR2RGB))\n",
    "#         plt.show()        \n",
    "        return final\n",
    "        \n",
    "    left_img, center_img, right_img = imgs[0], imgs[1], imgs[2]\n",
    "#     center_img = \n",
    "    center_right,t = stitch_right(center_img, right_img)\n",
    "#     print('center right')\n",
    "#     plt.imshow(cv2.cvtColor(center_right.astype('uint8'), cv2.COLOR_BGR2RGB))\n",
    "#     plt.show()\n",
    "    new_left = stitch_left(center_img, left_img, t, center_right)\n",
    "#     print('center left')\n",
    "#     plt.imshow(cv2.cvtColor(new_left.astype('uint8'), cv2.COLOR_BGR2RGB))\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Input:\n",
    "    #     *imgs: Arbitrary number of images, each of shape (m, n, 3)\n",
    "    # Output:\n",
    "    #     stitched_img: image comprised of all the images stitched together, of shape (>=m, >=n, 3).\n",
    "    \n",
    "    return new_left\n",
    "\n",
    "# center_img = cv2.imread(\"mountain_center.png\")\n",
    "# left_img = cv2.imread(\"mountain_left.png\")\n",
    "# right_img = cv2.imread(\"mountain_right.png\")\n",
    "\n",
    "################### MODIFY THIS LINE ###################\n",
    "# final_img = stitch_img(left_img, center_img, right_img)\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    }
   ],
   "source": [
    "center_img = cv2.imread(\"mountain_center.png\")\n",
    "left_img = cv2.imread(\"mountain_left.png\")\n",
    "right_img = cv2.imread(\"mountain_right.png\")\n",
    "\n",
    "\n",
    "\n",
    "################### MODIFY THIS LINE ###################\n",
    "final_img = stitch_img(left_img, center_img, right_img)\n",
    "########################################################\n",
    "\n",
    "plt.imshow(cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "write_output_img(\"test5.png\", final_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Testing your app (4 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture **5 (or more) images** using your own camera and stitch them to create a mosaic. Note that the mosaic need not be a horizontal panorama. Submit both the captured and stitched images. When you submit, make sure you run the notebook through so we can visually check your results. Also make sure to submit the images you used, as well as the output stitched image, all inside of a folder titled `part6/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**TODO:**</span> Load in your own pictures, run `stitch_img`, and show your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    }
   ],
   "source": [
    "img_1 = cv2.imread(\"S1.png\")\n",
    "img_2 = cv2.imread(\"S2.png\")\n",
    "img_3 = cv2.imread(\"S3.png\")\n",
    "img_4 = cv2.imread(\"S4.png\")\n",
    "img_5 = cv2.imread(\"S5.png\")\n",
    "\n",
    "\n",
    "stitched_img = stitch_img(img_1, img_2, img_3, img_4, img_5)\n",
    "\n",
    "\n",
    "# ...\n",
    "# img_n = cv2.imread(\"img_n.png\")\n",
    "\n",
    "# stitched_img = stitch_img(img_1, img_2, ..., img_n)\n",
    "plt.imshow(cv2.cvtColor(stitched_img, cv2.COLOR_BGR2RGB))\n",
    "write_output_img(\"test6.png\", stitched_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
